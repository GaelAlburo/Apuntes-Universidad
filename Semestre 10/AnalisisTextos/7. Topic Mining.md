Tópico - Aquello de lo que esta hablando (texto, música, película, etc.)

Mientras mas grande el texto, puede tener mas tópicos o se repite el tópico.

Problemas de tópico-termino:
- Falta de poder de expresividad
- Falta de cobertura de vocabulario
- Ambigüedad de sentido

Objetivo Topic Mining: Encontrar una estructura dentro de un texto

Topico: Distribucion de terminos. El topico ahora son muchas palabras. DIstribucion del vocabulario

Cada documento es una distribucion distinta de las distribuciones del vocabulario.

$\sum_{w \in V} p(w|\theta_i) = 1$

Input:
- Coleccion $C$ de $N$ documentos $d$  :  $C = \{ d_1, ..., d_N \}$
- Vocabulario $V$  :  $V=\{ w_1, ..., w_N \}$
- Numero de topicos  $k$
Output:
- $k$ topicos (distribuciones del vocabulario)  $\{ \theta_1, ..., \theta_k \},  \sum_{w \in V} p(w|\theta_i) = 1$
- Cobertura de topicos en cada $d_i$  $\{ \pi_{i1}, ..., \pi_{ik} \} \sum_{j=1}^{k} \pi_{ij} = 1$
	- $\pi_{ij}$  :  probabilidad de $\theta_j$ en $d_i$



$P(Datos | Modelo, \lambda)$
$\lambda = \Big( \{\theta_1, ..., \theta_k\}, \{\pi_{11}, ..., \pi_{1k}\}, ..., \{\pi_{N1}, ..., \pi_{Nk}\} \Big)$
 
 Lo que se busca es maximizar a $\lambda$:
$\lambda^{*} = argmax \,\,\, p(Data | Model, \lambda)$

Modelo de lengua de unigramas:
- Cada token tiene una probabilidad
- La probabilidad de una secuencia de tokens es el producto de las probabilidades que la conforman (independencia)

Modelo de unigramas 1:
- texto - 0.2
- minería - 0.1
- asociación - 0.02
- agrupamiento - 0.01
- ...
- comida - 0.000001

Modelo de unigramas 2:
- comida - 0.25
- nutrición - 0.1
- saludable - 0.05
- ...

Para generar un modelo de unigramas - Estimado de verosimilitud maxima (MLE)
- Se realiza el conteo de palabras y se divide entre el total de palabras
	- Con 100 palabras en $d$ documento:
		- texto - 10
		- mineria - 5
		- algoritmo - 1
	- Las probabilidades son:
		- texto - 0.1
		- mineria - 0.05
		- algoritmo - 0.01


**MLE**
Mejor: verosimilitud de los datos alcanzan el punto maximo
$$\hat{\theta} = argmax \,\, P(x|\theta)$$
Problema - tamano de la muestra

**Bayesiana**
Mejor: consistencia con el conocimiento previo (*prior*) y explicacion de los datos
(Se asume la existencia de datos)
$$\hat{\theta} = argmax \,\, P(\theta|x) = argmax \,\, P(x|\theta)\, P(\theta) $$
Problema - Como se define a *prior*?

___
pLSA
- No-generativo
	- Determina como se generaron los datos, pero no puede generar

___
DIagrama de platos

Solo se calculan los parametros $\alpha$ y $\beta$ 

$\theta$: topico
$\phi$: distribucion del topico
 z: nos permite elegir la palabra exacta a obtener
 N: numero de palabras del documento especifico,,

Con mover los parametros obtenemos la informacion deseada